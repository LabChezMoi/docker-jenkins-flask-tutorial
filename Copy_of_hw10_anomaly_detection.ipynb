{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of hw10_anomaly_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LabChezMoi/docker-jenkins-flask-tutorial/blob/master/Copy_of_hw10_anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiVfKn-6tXz8",
        "colab_type": "text"
      },
      "source": [
        "# **Homework 10 - Anomaly Detection**\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qLcLuhETSDH",
        "colab_type": "text"
      },
      "source": [
        "# Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBo2oxu_WmZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "b4bbb8f8-c28a-416c-a905-3990a4ba5a12"
      },
      "source": [
        "!gdown --id '1_zT3JOpvXFGr7mkxs3XJDeGxTn_8pItq' --output train.npy \n",
        "!gdown --id '11Y_6JDjlhIY-M5-jW1rLRshDMqeKi9Kr' --output test.npy \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "train = np.load('train.npy', allow_pickle=True)\n",
        "test = np.load('test.npy', allow_pickle=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_zT3JOpvXFGr7mkxs3XJDeGxTn_8pItq\n",
            "To: /content/train.npy\n",
            "983MB [00:13, 74.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11Y_6JDjlhIY-M5-jW1rLRshDMqeKi9Kr\n",
            "To: /content/test.npy\n",
            "246MB [00:04, 59.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQxNNeftxx_y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfa7eb7b-f88a-4b11-9b5c-c0d82ae4671c"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrpPP6x3XHo_",
        "colab_type": "text"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTcv510uhi6",
        "colab_type": "text"
      },
      "source": [
        "這份作業要執行的 task 是 semi-supervised anomaly detection，也就是說 training set 是乾淨的，testing 的時候才會混進 outlier data (anomaly)。\n",
        "我們以某個簡單的 image dataset（image 加上他們的 label（分類））作為示範，training data 為原先 training set 中的某幾類，而 testing data 則是原先 testing set 的所有 data，要偵測的 anomaly 為 training data 中未出現的類別。label 的部分，1 為 outlier data，而 0 為 inlier data（相對於 outlier）。正確率以 AUC 計算。\n",
        "\n",
        "方法以下列舉 3 種： K-means, PCA, Autoencoder。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHQqB7IXOXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = 'pca'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4GwvK4DXuW4",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAIl7IETpbP2",
        "colab_type": "text"
      },
      "source": [
        "K-means: 假設 training data 的 label 種類不多（e.g., < 20），然而因其為未知，可以猜測其為 n，亦即假設 training data 有 n 群。先用 K-means 計算 training data 中的 n 個 centroid，再用這 n 個 centroid 對 training data 分群。應該可以觀察到，inlier data 與所分到群的 centroid 的距離應較 outlier 的此距離來得小。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7AB9wnaX1To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import f1_score, pairwise_distances, roc_auc_score\n",
        "from scipy.cluster.vq import vq, kmeans\n",
        "\n",
        "\n",
        "if task == 'knn':\n",
        "    x = train.reshape(len(train), -1)\n",
        "    y = test.reshape(len(test), -1)\n",
        "    scores = list()\n",
        "    for n in range(1, 10):\n",
        "      kmeans_x = MiniBatchKMeans(n_clusters=n, batch_size=100).fit(x)\n",
        "      y_cluster = kmeans_x.predict(y)\n",
        "      y_dist = np.sum(np.square(kmeans_x.cluster_centers_[y_cluster] - y), axis=1)\n",
        "\n",
        "      y_pred = y_dist\n",
        "      # score = f1_score(y_label, y_pred, average='micro')\n",
        "      # score = roc_auc_score(y_label, y_pred, average='micro')\n",
        "      # scores.append(score)\n",
        "    # print(np.max(scores), np.argmax(scores))\n",
        "    # print(scores)\n",
        "    # print('auc score: {}'.format(np.max(scores)))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLScetmgf4XV",
        "colab_type": "text"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZKv6vNYMiWA",
        "colab_type": "text"
      },
      "source": [
        "PCA: 首先計算 training data 的 principle component，將 testing data 投影在這些 component 上，再將這些投影重建回原先 space 的向量。對重建的圖片和原圖計算 MSE，inlier data 的數值應該較 outlier 的數值為小。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzgZGG7Pf6Qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "if task == 'pca':\n",
        "\n",
        "    x = train.reshape(len(train), -1)\n",
        "    y = test.reshape(len(test), -1)\n",
        "    pca = PCA(n_components=2).fit(x)\n",
        "\n",
        "    y_projected = pca.transform(y)\n",
        "    y_reconstructed = pca.inverse_transform(y_projected)  \n",
        "    dist = np.sqrt(np.sum(np.square(y_reconstructed - y).reshape(len(y), -1), axis=1))\n",
        "    \n",
        "    y_pred = dist\n",
        "    # score = roc_auc_score(y_label, y_pred, average='micro')\n",
        "    # score = f1_score(y_label, y_pred, average='micro')\n",
        "    # print('auc score: {}'.format(score))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR9zC0_Df-CR",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EbfwRREhA7c",
        "colab_type": "text"
      },
      "source": [
        "# Models & loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUUIwFm02qEL",
        "colab_type": "text"
      },
      "source": [
        "課程影片參見：https://www.youtube.com/watch?v=6W8FqUGYyDo&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lESPM7C9svYa",
        "colab_type": "text"
      },
      "source": [
        "fcn_autoencoder and vae are from https://github.com/L1aoXingyu/pytorch-beginner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwG2XzgWs_JR",
        "colab_type": "text"
      },
      "source": [
        "conv_autoencoder is from https://github.com/jellycsc/PyTorch-CIFAR-10-autoencoder/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpExFuC2VoX6",
        "colab_type": "text"
      },
      "source": [
        "同學們可以嘗試各種不同的 VAE 架構"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi8ds1fugCkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class fcn_autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(fcn_autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(32 * 32 * 3, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3, 12),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(12, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(True), nn.Linear(128, 32 * 32 * 3\n",
        "            ), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class conv_autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(conv_autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
        "            nn.ReLU(),\n",
        "\t\t\t      nn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
        "            nn.ReLU(),\n",
        "    # \t\t\tnn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]\n",
        "    #       nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
        "#             nn.ReLU(),\n",
        "\t\t\t      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
        "            nn.ReLU(),\n",
        "\t\t\t      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(32*32*3, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 32*32*3)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        if torch.cuda.is_available():\n",
        "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "        else:\n",
        "            eps = torch.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return F.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "def loss_vae(recon_x, x, mu, logvar, criterion):\n",
        "    \"\"\"\n",
        "    recon_x: generating images\n",
        "    x: origin images\n",
        "    mu: latent mean\n",
        "    logvar: latent log variance\n",
        "    \"\"\"\n",
        "    mse = criterion(recon_x, x)  # mse loss\n",
        "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
        "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
        "    # KL divergence\n",
        "    return mse + KLD\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKNUImqUhIeq",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoW1UrrxgI_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be5039f6-444e-4f65-8960-d9be570718d5"
      },
      "source": [
        "task = 'ae'\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "\n",
        "\n",
        "if task == 'ae':\n",
        "    num_epochs = 1000\n",
        "    batch_size = 128\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    #{'fcn', 'cnn', 'vae'} \n",
        "    model_type = 'cnn' \n",
        "\n",
        "    x = train\n",
        "    if model_type == 'fcn' or model_type == 'vae':\n",
        "        x = x.reshape(len(x), -1)\n",
        "        \n",
        "    data = torch.tensor(x, dtype=torch.float)\n",
        "    train_dataset = TensorDataset(data)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    model_classes = {'fcn':fcn_autoencoder(), 'cnn':conv_autoencoder(), 'vae':VAE()}\n",
        "    model = model_classes[model_type].cuda()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    best_loss = np.inf\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_dataloader:\n",
        "            if model_type == 'cnn':\n",
        "                img = data[0].transpose(3, 1).cuda()\n",
        "            else:\n",
        "                img = data[0].cuda()\n",
        "            # ===================forward=====================\n",
        "            output = model(img)\n",
        "            if model_type == 'vae':\n",
        "                loss = loss_vae(output[0], img, output[1], output[2], criterion)\n",
        "            else:\n",
        "                loss = criterion(output, img)\n",
        "            # ===================backward====================\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # ===================save====================\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                torch.save(model, 'best_model_{}.pt'.format(model_type))\n",
        "        # ===================log========================\n",
        "        print('epoch [{}/{}], loss:{:.4f}'\n",
        "              .format(epoch + 1, num_epochs, loss.item()))\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch [1/1000], loss:0.0388\n",
            "epoch [2/1000], loss:0.0243\n",
            "epoch [3/1000], loss:0.0216\n",
            "epoch [4/1000], loss:0.0164\n",
            "epoch [5/1000], loss:0.0162\n",
            "epoch [6/1000], loss:0.0140\n",
            "epoch [7/1000], loss:0.0140\n",
            "epoch [8/1000], loss:0.0131\n",
            "epoch [9/1000], loss:0.0120\n",
            "epoch [10/1000], loss:0.0125\n",
            "epoch [11/1000], loss:0.0114\n",
            "epoch [12/1000], loss:0.0112\n",
            "epoch [13/1000], loss:0.0121\n",
            "epoch [14/1000], loss:0.0127\n",
            "epoch [15/1000], loss:0.0118\n",
            "epoch [16/1000], loss:0.0104\n",
            "epoch [17/1000], loss:0.0108\n",
            "epoch [18/1000], loss:0.0107\n",
            "epoch [19/1000], loss:0.0117\n",
            "epoch [20/1000], loss:0.0094\n",
            "epoch [21/1000], loss:0.0091\n",
            "epoch [22/1000], loss:0.0101\n",
            "epoch [23/1000], loss:0.0085\n",
            "epoch [24/1000], loss:0.0090\n",
            "epoch [25/1000], loss:0.0086\n",
            "epoch [26/1000], loss:0.0074\n",
            "epoch [27/1000], loss:0.0086\n",
            "epoch [28/1000], loss:0.0113\n",
            "epoch [29/1000], loss:0.0071\n",
            "epoch [30/1000], loss:0.0074\n",
            "epoch [31/1000], loss:0.0075\n",
            "epoch [32/1000], loss:0.0074\n",
            "epoch [33/1000], loss:0.0064\n",
            "epoch [34/1000], loss:0.0070\n",
            "epoch [35/1000], loss:0.0074\n",
            "epoch [36/1000], loss:0.0084\n",
            "epoch [37/1000], loss:0.0066\n",
            "epoch [38/1000], loss:0.0062\n",
            "epoch [39/1000], loss:0.0070\n",
            "epoch [40/1000], loss:0.0060\n",
            "epoch [41/1000], loss:0.0061\n",
            "epoch [42/1000], loss:0.0057\n",
            "epoch [43/1000], loss:0.0065\n",
            "epoch [44/1000], loss:0.0054\n",
            "epoch [45/1000], loss:0.0065\n",
            "epoch [46/1000], loss:0.0060\n",
            "epoch [47/1000], loss:0.0062\n",
            "epoch [48/1000], loss:0.0062\n",
            "epoch [49/1000], loss:0.0056\n",
            "epoch [50/1000], loss:0.0051\n",
            "epoch [51/1000], loss:0.0056\n",
            "epoch [52/1000], loss:0.0068\n",
            "epoch [53/1000], loss:0.0054\n",
            "epoch [54/1000], loss:0.0060\n",
            "epoch [55/1000], loss:0.0046\n",
            "epoch [56/1000], loss:0.0054\n",
            "epoch [57/1000], loss:0.0054\n",
            "epoch [58/1000], loss:0.0057\n",
            "epoch [59/1000], loss:0.0053\n",
            "epoch [60/1000], loss:0.0050\n",
            "epoch [61/1000], loss:0.0046\n",
            "epoch [62/1000], loss:0.0057\n",
            "epoch [63/1000], loss:0.0047\n",
            "epoch [64/1000], loss:0.0050\n",
            "epoch [65/1000], loss:0.0052\n",
            "epoch [66/1000], loss:0.0052\n",
            "epoch [67/1000], loss:0.0051\n",
            "epoch [68/1000], loss:0.0050\n",
            "epoch [69/1000], loss:0.0053\n",
            "epoch [70/1000], loss:0.0046\n",
            "epoch [71/1000], loss:0.0041\n",
            "epoch [72/1000], loss:0.0047\n",
            "epoch [73/1000], loss:0.0044\n",
            "epoch [74/1000], loss:0.0046\n",
            "epoch [75/1000], loss:0.0042\n",
            "epoch [76/1000], loss:0.0044\n",
            "epoch [77/1000], loss:0.0036\n",
            "epoch [78/1000], loss:0.0045\n",
            "epoch [79/1000], loss:0.0040\n",
            "epoch [80/1000], loss:0.0038\n",
            "epoch [81/1000], loss:0.0040\n",
            "epoch [82/1000], loss:0.0041\n",
            "epoch [83/1000], loss:0.0043\n",
            "epoch [84/1000], loss:0.0046\n",
            "epoch [85/1000], loss:0.0040\n",
            "epoch [86/1000], loss:0.0041\n",
            "epoch [87/1000], loss:0.0042\n",
            "epoch [88/1000], loss:0.0036\n",
            "epoch [89/1000], loss:0.0050\n",
            "epoch [90/1000], loss:0.0047\n",
            "epoch [91/1000], loss:0.0038\n",
            "epoch [92/1000], loss:0.0040\n",
            "epoch [93/1000], loss:0.0051\n",
            "epoch [94/1000], loss:0.0043\n",
            "epoch [95/1000], loss:0.0044\n",
            "epoch [96/1000], loss:0.0047\n",
            "epoch [97/1000], loss:0.0045\n",
            "epoch [98/1000], loss:0.0042\n",
            "epoch [99/1000], loss:0.0044\n",
            "epoch [100/1000], loss:0.0042\n",
            "epoch [101/1000], loss:0.0038\n",
            "epoch [102/1000], loss:0.0036\n",
            "epoch [103/1000], loss:0.0042\n",
            "epoch [104/1000], loss:0.0044\n",
            "epoch [105/1000], loss:0.0066\n",
            "epoch [106/1000], loss:0.0045\n",
            "epoch [107/1000], loss:0.0043\n",
            "epoch [108/1000], loss:0.0044\n",
            "epoch [109/1000], loss:0.0041\n",
            "epoch [110/1000], loss:0.0045\n",
            "epoch [111/1000], loss:0.0042\n",
            "epoch [112/1000], loss:0.0045\n",
            "epoch [113/1000], loss:0.0037\n",
            "epoch [114/1000], loss:0.0036\n",
            "epoch [115/1000], loss:0.0048\n",
            "epoch [116/1000], loss:0.0034\n",
            "epoch [117/1000], loss:0.0036\n",
            "epoch [118/1000], loss:0.0037\n",
            "epoch [119/1000], loss:0.0037\n",
            "epoch [120/1000], loss:0.0039\n",
            "epoch [121/1000], loss:0.0036\n",
            "epoch [122/1000], loss:0.0042\n",
            "epoch [123/1000], loss:0.0034\n",
            "epoch [124/1000], loss:0.0041\n",
            "epoch [125/1000], loss:0.0035\n",
            "epoch [126/1000], loss:0.0035\n",
            "epoch [127/1000], loss:0.0035\n",
            "epoch [128/1000], loss:0.0037\n",
            "epoch [129/1000], loss:0.0041\n",
            "epoch [130/1000], loss:0.0039\n",
            "epoch [131/1000], loss:0.0039\n",
            "epoch [132/1000], loss:0.0041\n",
            "epoch [133/1000], loss:0.0034\n",
            "epoch [134/1000], loss:0.0033\n",
            "epoch [135/1000], loss:0.0037\n",
            "epoch [136/1000], loss:0.0043\n",
            "epoch [137/1000], loss:0.0039\n",
            "epoch [138/1000], loss:0.0042\n",
            "epoch [139/1000], loss:0.0037\n",
            "epoch [140/1000], loss:0.0037\n",
            "epoch [141/1000], loss:0.0037\n",
            "epoch [142/1000], loss:0.0039\n",
            "epoch [143/1000], loss:0.0037\n",
            "epoch [144/1000], loss:0.0037\n",
            "epoch [145/1000], loss:0.0033\n",
            "epoch [146/1000], loss:0.0048\n",
            "epoch [147/1000], loss:0.0040\n",
            "epoch [148/1000], loss:0.0037\n",
            "epoch [149/1000], loss:0.0037\n",
            "epoch [150/1000], loss:0.0041\n",
            "epoch [151/1000], loss:0.0038\n",
            "epoch [152/1000], loss:0.0031\n",
            "epoch [153/1000], loss:0.0040\n",
            "epoch [154/1000], loss:0.0035\n",
            "epoch [155/1000], loss:0.0032\n",
            "epoch [156/1000], loss:0.0029\n",
            "epoch [157/1000], loss:0.0032\n",
            "epoch [158/1000], loss:0.0029\n",
            "epoch [159/1000], loss:0.0039\n",
            "epoch [160/1000], loss:0.0037\n",
            "epoch [161/1000], loss:0.0032\n",
            "epoch [162/1000], loss:0.0042\n",
            "epoch [163/1000], loss:0.0035\n",
            "epoch [164/1000], loss:0.0033\n",
            "epoch [165/1000], loss:0.0040\n",
            "epoch [166/1000], loss:0.0033\n",
            "epoch [167/1000], loss:0.0031\n",
            "epoch [168/1000], loss:0.0037\n",
            "epoch [169/1000], loss:0.0033\n",
            "epoch [170/1000], loss:0.0036\n",
            "epoch [171/1000], loss:0.0034\n",
            "epoch [172/1000], loss:0.0040\n",
            "epoch [173/1000], loss:0.0030\n",
            "epoch [174/1000], loss:0.0034\n",
            "epoch [175/1000], loss:0.0034\n",
            "epoch [176/1000], loss:0.0034\n",
            "epoch [177/1000], loss:0.0034\n",
            "epoch [178/1000], loss:0.0033\n",
            "epoch [179/1000], loss:0.0038\n",
            "epoch [180/1000], loss:0.0035\n",
            "epoch [181/1000], loss:0.0034\n",
            "epoch [182/1000], loss:0.0030\n",
            "epoch [183/1000], loss:0.0036\n",
            "epoch [184/1000], loss:0.0033\n",
            "epoch [185/1000], loss:0.0032\n",
            "epoch [186/1000], loss:0.0031\n",
            "epoch [187/1000], loss:0.0035\n",
            "epoch [188/1000], loss:0.0033\n",
            "epoch [189/1000], loss:0.0034\n",
            "epoch [190/1000], loss:0.0031\n",
            "epoch [191/1000], loss:0.0031\n",
            "epoch [192/1000], loss:0.0030\n",
            "epoch [193/1000], loss:0.0037\n",
            "epoch [194/1000], loss:0.0030\n",
            "epoch [195/1000], loss:0.0031\n",
            "epoch [196/1000], loss:0.0040\n",
            "epoch [197/1000], loss:0.0033\n",
            "epoch [198/1000], loss:0.0031\n",
            "epoch [199/1000], loss:0.0032\n",
            "epoch [200/1000], loss:0.0033\n",
            "epoch [201/1000], loss:0.0036\n",
            "epoch [202/1000], loss:0.0041\n",
            "epoch [203/1000], loss:0.0034\n",
            "epoch [204/1000], loss:0.0039\n",
            "epoch [205/1000], loss:0.0030\n",
            "epoch [206/1000], loss:0.0033\n",
            "epoch [207/1000], loss:0.0032\n",
            "epoch [208/1000], loss:0.0033\n",
            "epoch [209/1000], loss:0.0033\n",
            "epoch [210/1000], loss:0.0028\n",
            "epoch [211/1000], loss:0.0036\n",
            "epoch [212/1000], loss:0.0029\n",
            "epoch [213/1000], loss:0.0028\n",
            "epoch [214/1000], loss:0.0030\n",
            "epoch [215/1000], loss:0.0030\n",
            "epoch [216/1000], loss:0.0030\n",
            "epoch [217/1000], loss:0.0029\n",
            "epoch [218/1000], loss:0.0030\n",
            "epoch [219/1000], loss:0.0028\n",
            "epoch [220/1000], loss:0.0032\n",
            "epoch [221/1000], loss:0.0032\n",
            "epoch [222/1000], loss:0.0029\n",
            "epoch [223/1000], loss:0.0031\n",
            "epoch [224/1000], loss:0.0028\n",
            "epoch [225/1000], loss:0.0035\n",
            "epoch [226/1000], loss:0.0043\n",
            "epoch [227/1000], loss:0.0027\n",
            "epoch [228/1000], loss:0.0032\n",
            "epoch [229/1000], loss:0.0037\n",
            "epoch [230/1000], loss:0.0028\n",
            "epoch [231/1000], loss:0.0032\n",
            "epoch [232/1000], loss:0.0031\n",
            "epoch [233/1000], loss:0.0030\n",
            "epoch [234/1000], loss:0.0031\n",
            "epoch [235/1000], loss:0.0028\n",
            "epoch [236/1000], loss:0.0028\n",
            "epoch [237/1000], loss:0.0027\n",
            "epoch [238/1000], loss:0.0029\n",
            "epoch [239/1000], loss:0.0028\n",
            "epoch [240/1000], loss:0.0030\n",
            "epoch [241/1000], loss:0.0029\n",
            "epoch [242/1000], loss:0.0032\n",
            "epoch [243/1000], loss:0.0031\n",
            "epoch [244/1000], loss:0.0031\n",
            "epoch [245/1000], loss:0.0039\n",
            "epoch [246/1000], loss:0.0042\n",
            "epoch [247/1000], loss:0.0034\n",
            "epoch [248/1000], loss:0.0030\n",
            "epoch [249/1000], loss:0.0030\n",
            "epoch [250/1000], loss:0.0028\n",
            "epoch [251/1000], loss:0.0029\n",
            "epoch [252/1000], loss:0.0030\n",
            "epoch [253/1000], loss:0.0036\n",
            "epoch [254/1000], loss:0.0033\n",
            "epoch [255/1000], loss:0.0036\n",
            "epoch [256/1000], loss:0.0030\n",
            "epoch [257/1000], loss:0.0032\n",
            "epoch [258/1000], loss:0.0029\n",
            "epoch [259/1000], loss:0.0030\n",
            "epoch [260/1000], loss:0.0034\n",
            "epoch [261/1000], loss:0.0031\n",
            "epoch [262/1000], loss:0.0031\n",
            "epoch [263/1000], loss:0.0029\n",
            "epoch [264/1000], loss:0.0032\n",
            "epoch [265/1000], loss:0.0028\n",
            "epoch [266/1000], loss:0.0029\n",
            "epoch [267/1000], loss:0.0031\n",
            "epoch [268/1000], loss:0.0031\n",
            "epoch [269/1000], loss:0.0033\n",
            "epoch [270/1000], loss:0.0029\n",
            "epoch [271/1000], loss:0.0033\n",
            "epoch [272/1000], loss:0.0036\n",
            "epoch [273/1000], loss:0.0035\n",
            "epoch [274/1000], loss:0.0028\n",
            "epoch [275/1000], loss:0.0030\n",
            "epoch [276/1000], loss:0.0027\n",
            "epoch [277/1000], loss:0.0029\n",
            "epoch [278/1000], loss:0.0030\n",
            "epoch [279/1000], loss:0.0035\n",
            "epoch [280/1000], loss:0.0030\n",
            "epoch [281/1000], loss:0.0031\n",
            "epoch [282/1000], loss:0.0029\n",
            "epoch [283/1000], loss:0.0033\n",
            "epoch [284/1000], loss:0.0034\n",
            "epoch [285/1000], loss:0.0030\n",
            "epoch [286/1000], loss:0.0031\n",
            "epoch [287/1000], loss:0.0031\n",
            "epoch [288/1000], loss:0.0031\n",
            "epoch [289/1000], loss:0.0032\n",
            "epoch [290/1000], loss:0.0031\n",
            "epoch [291/1000], loss:0.0030\n",
            "epoch [292/1000], loss:0.0033\n",
            "epoch [293/1000], loss:0.0031\n",
            "epoch [294/1000], loss:0.0038\n",
            "epoch [295/1000], loss:0.0031\n",
            "epoch [296/1000], loss:0.0027\n",
            "epoch [297/1000], loss:0.0025\n",
            "epoch [298/1000], loss:0.0031\n",
            "epoch [299/1000], loss:0.0040\n",
            "epoch [300/1000], loss:0.0029\n",
            "epoch [301/1000], loss:0.0032\n",
            "epoch [302/1000], loss:0.0032\n",
            "epoch [303/1000], loss:0.0031\n",
            "epoch [304/1000], loss:0.0027\n",
            "epoch [305/1000], loss:0.0025\n",
            "epoch [306/1000], loss:0.0028\n",
            "epoch [307/1000], loss:0.0027\n",
            "epoch [308/1000], loss:0.0031\n",
            "epoch [309/1000], loss:0.0032\n",
            "epoch [310/1000], loss:0.0027\n",
            "epoch [311/1000], loss:0.0032\n",
            "epoch [312/1000], loss:0.0030\n",
            "epoch [313/1000], loss:0.0029\n",
            "epoch [314/1000], loss:0.0026\n",
            "epoch [315/1000], loss:0.0031\n",
            "epoch [316/1000], loss:0.0034\n",
            "epoch [317/1000], loss:0.0031\n",
            "epoch [318/1000], loss:0.0033\n",
            "epoch [319/1000], loss:0.0040\n",
            "epoch [320/1000], loss:0.0032\n",
            "epoch [321/1000], loss:0.0028\n",
            "epoch [322/1000], loss:0.0028\n",
            "epoch [323/1000], loss:0.0028\n",
            "epoch [324/1000], loss:0.0032\n",
            "epoch [325/1000], loss:0.0028\n",
            "epoch [326/1000], loss:0.0036\n",
            "epoch [327/1000], loss:0.0031\n",
            "epoch [328/1000], loss:0.0027\n",
            "epoch [329/1000], loss:0.0034\n",
            "epoch [330/1000], loss:0.0027\n",
            "epoch [331/1000], loss:0.0029\n",
            "epoch [332/1000], loss:0.0030\n",
            "epoch [333/1000], loss:0.0030\n",
            "epoch [334/1000], loss:0.0027\n",
            "epoch [335/1000], loss:0.0029\n",
            "epoch [336/1000], loss:0.0029\n",
            "epoch [337/1000], loss:0.0032\n",
            "epoch [338/1000], loss:0.0030\n",
            "epoch [339/1000], loss:0.0026\n",
            "epoch [340/1000], loss:0.0027\n",
            "epoch [341/1000], loss:0.0032\n",
            "epoch [342/1000], loss:0.0027\n",
            "epoch [343/1000], loss:0.0031\n",
            "epoch [344/1000], loss:0.0028\n",
            "epoch [345/1000], loss:0.0033\n",
            "epoch [346/1000], loss:0.0031\n",
            "epoch [347/1000], loss:0.0030\n",
            "epoch [348/1000], loss:0.0032\n",
            "epoch [349/1000], loss:0.0030\n",
            "epoch [350/1000], loss:0.0031\n",
            "epoch [351/1000], loss:0.0032\n",
            "epoch [352/1000], loss:0.0031\n",
            "epoch [353/1000], loss:0.0033\n",
            "epoch [354/1000], loss:0.0029\n",
            "epoch [355/1000], loss:0.0027\n",
            "epoch [356/1000], loss:0.0030\n",
            "epoch [357/1000], loss:0.0030\n",
            "epoch [358/1000], loss:0.0033\n",
            "epoch [359/1000], loss:0.0027\n",
            "epoch [360/1000], loss:0.0033\n",
            "epoch [361/1000], loss:0.0029\n",
            "epoch [362/1000], loss:0.0029\n",
            "epoch [363/1000], loss:0.0033\n",
            "epoch [364/1000], loss:0.0036\n",
            "epoch [365/1000], loss:0.0024\n",
            "epoch [366/1000], loss:0.0030\n",
            "epoch [367/1000], loss:0.0029\n",
            "epoch [368/1000], loss:0.0027\n",
            "epoch [369/1000], loss:0.0030\n",
            "epoch [370/1000], loss:0.0029\n",
            "epoch [371/1000], loss:0.0030\n",
            "epoch [372/1000], loss:0.0027\n",
            "epoch [373/1000], loss:0.0032\n",
            "epoch [374/1000], loss:0.0032\n",
            "epoch [375/1000], loss:0.0029\n",
            "epoch [376/1000], loss:0.0034\n",
            "epoch [377/1000], loss:0.0030\n",
            "epoch [378/1000], loss:0.0028\n",
            "epoch [379/1000], loss:0.0026\n",
            "epoch [380/1000], loss:0.0037\n",
            "epoch [381/1000], loss:0.0026\n",
            "epoch [382/1000], loss:0.0035\n",
            "epoch [383/1000], loss:0.0031\n",
            "epoch [384/1000], loss:0.0029\n",
            "epoch [385/1000], loss:0.0026\n",
            "epoch [386/1000], loss:0.0035\n",
            "epoch [387/1000], loss:0.0025\n",
            "epoch [388/1000], loss:0.0035\n",
            "epoch [389/1000], loss:0.0030\n",
            "epoch [390/1000], loss:0.0028\n",
            "epoch [391/1000], loss:0.0028\n",
            "epoch [392/1000], loss:0.0029\n",
            "epoch [393/1000], loss:0.0030\n",
            "epoch [394/1000], loss:0.0027\n",
            "epoch [395/1000], loss:0.0033\n",
            "epoch [396/1000], loss:0.0032\n",
            "epoch [397/1000], loss:0.0036\n",
            "epoch [398/1000], loss:0.0034\n",
            "epoch [399/1000], loss:0.0029\n",
            "epoch [400/1000], loss:0.0034\n",
            "epoch [401/1000], loss:0.0038\n",
            "epoch [402/1000], loss:0.0027\n",
            "epoch [403/1000], loss:0.0029\n",
            "epoch [404/1000], loss:0.0035\n",
            "epoch [405/1000], loss:0.0030\n",
            "epoch [406/1000], loss:0.0029\n",
            "epoch [407/1000], loss:0.0028\n",
            "epoch [408/1000], loss:0.0033\n",
            "epoch [409/1000], loss:0.0025\n",
            "epoch [410/1000], loss:0.0032\n",
            "epoch [411/1000], loss:0.0031\n",
            "epoch [412/1000], loss:0.0037\n",
            "epoch [413/1000], loss:0.0032\n",
            "epoch [414/1000], loss:0.0036\n",
            "epoch [415/1000], loss:0.0036\n",
            "epoch [416/1000], loss:0.0030\n",
            "epoch [417/1000], loss:0.0027\n",
            "epoch [418/1000], loss:0.0030\n",
            "epoch [419/1000], loss:0.0029\n",
            "epoch [420/1000], loss:0.0026\n",
            "epoch [421/1000], loss:0.0026\n",
            "epoch [422/1000], loss:0.0036\n",
            "epoch [423/1000], loss:0.0028\n",
            "epoch [424/1000], loss:0.0033\n",
            "epoch [425/1000], loss:0.0028\n",
            "epoch [426/1000], loss:0.0029\n",
            "epoch [427/1000], loss:0.0029\n",
            "epoch [428/1000], loss:0.0027\n",
            "epoch [429/1000], loss:0.0029\n",
            "epoch [430/1000], loss:0.0031\n",
            "epoch [431/1000], loss:0.0027\n",
            "epoch [432/1000], loss:0.0029\n",
            "epoch [433/1000], loss:0.0030\n",
            "epoch [434/1000], loss:0.0033\n",
            "epoch [435/1000], loss:0.0026\n",
            "epoch [436/1000], loss:0.0029\n",
            "epoch [437/1000], loss:0.0024\n",
            "epoch [438/1000], loss:0.0027\n",
            "epoch [439/1000], loss:0.0026\n",
            "epoch [440/1000], loss:0.0031\n",
            "epoch [441/1000], loss:0.0030\n",
            "epoch [442/1000], loss:0.0031\n",
            "epoch [443/1000], loss:0.0031\n",
            "epoch [444/1000], loss:0.0030\n",
            "epoch [445/1000], loss:0.0028\n",
            "epoch [446/1000], loss:0.0029\n",
            "epoch [447/1000], loss:0.0031\n",
            "epoch [448/1000], loss:0.0033\n",
            "epoch [449/1000], loss:0.0033\n",
            "epoch [450/1000], loss:0.0028\n",
            "epoch [451/1000], loss:0.0033\n",
            "epoch [452/1000], loss:0.0031\n",
            "epoch [453/1000], loss:0.0028\n",
            "epoch [454/1000], loss:0.0026\n",
            "epoch [455/1000], loss:0.0029\n",
            "epoch [456/1000], loss:0.0028\n",
            "epoch [457/1000], loss:0.0030\n",
            "epoch [458/1000], loss:0.0024\n",
            "epoch [459/1000], loss:0.0029\n",
            "epoch [460/1000], loss:0.0030\n",
            "epoch [461/1000], loss:0.0027\n",
            "epoch [462/1000], loss:0.0029\n",
            "epoch [463/1000], loss:0.0029\n",
            "epoch [464/1000], loss:0.0031\n",
            "epoch [465/1000], loss:0.0032\n",
            "epoch [466/1000], loss:0.0030\n",
            "epoch [467/1000], loss:0.0033\n",
            "epoch [468/1000], loss:0.0032\n",
            "epoch [469/1000], loss:0.0034\n",
            "epoch [470/1000], loss:0.0033\n",
            "epoch [471/1000], loss:0.0027\n",
            "epoch [472/1000], loss:0.0031\n",
            "epoch [473/1000], loss:0.0041\n",
            "epoch [474/1000], loss:0.0030\n",
            "epoch [475/1000], loss:0.0027\n",
            "epoch [476/1000], loss:0.0029\n",
            "epoch [477/1000], loss:0.0033\n",
            "epoch [478/1000], loss:0.0028\n",
            "epoch [479/1000], loss:0.0030\n",
            "epoch [480/1000], loss:0.0031\n",
            "epoch [481/1000], loss:0.0031\n",
            "epoch [482/1000], loss:0.0029\n",
            "epoch [483/1000], loss:0.0027\n",
            "epoch [484/1000], loss:0.0029\n",
            "epoch [485/1000], loss:0.0028\n",
            "epoch [486/1000], loss:0.0028\n",
            "epoch [487/1000], loss:0.0026\n",
            "epoch [488/1000], loss:0.0029\n",
            "epoch [489/1000], loss:0.0034\n",
            "epoch [490/1000], loss:0.0029\n",
            "epoch [491/1000], loss:0.0031\n",
            "epoch [492/1000], loss:0.0028\n",
            "epoch [493/1000], loss:0.0037\n",
            "epoch [494/1000], loss:0.0031\n",
            "epoch [495/1000], loss:0.0030\n",
            "epoch [496/1000], loss:0.0030\n",
            "epoch [497/1000], loss:0.0028\n",
            "epoch [498/1000], loss:0.0029\n",
            "epoch [499/1000], loss:0.0030\n",
            "epoch [500/1000], loss:0.0029\n",
            "epoch [501/1000], loss:0.0031\n",
            "epoch [502/1000], loss:0.0027\n",
            "epoch [503/1000], loss:0.0033\n",
            "epoch [504/1000], loss:0.0027\n",
            "epoch [505/1000], loss:0.0031\n",
            "epoch [506/1000], loss:0.0031\n",
            "epoch [507/1000], loss:0.0032\n",
            "epoch [508/1000], loss:0.0034\n",
            "epoch [509/1000], loss:0.0035\n",
            "epoch [510/1000], loss:0.0025\n",
            "epoch [511/1000], loss:0.0030\n",
            "epoch [512/1000], loss:0.0027\n",
            "epoch [513/1000], loss:0.0030\n",
            "epoch [514/1000], loss:0.0029\n",
            "epoch [515/1000], loss:0.0030\n",
            "epoch [516/1000], loss:0.0028\n",
            "epoch [517/1000], loss:0.0033\n",
            "epoch [518/1000], loss:0.0030\n",
            "epoch [519/1000], loss:0.0037\n",
            "epoch [520/1000], loss:0.0033\n",
            "epoch [521/1000], loss:0.0022\n",
            "epoch [522/1000], loss:0.0030\n",
            "epoch [523/1000], loss:0.0032\n",
            "epoch [524/1000], loss:0.0032\n",
            "epoch [525/1000], loss:0.0028\n",
            "epoch [526/1000], loss:0.0027\n",
            "epoch [527/1000], loss:0.0027\n",
            "epoch [528/1000], loss:0.0029\n",
            "epoch [529/1000], loss:0.0030\n",
            "epoch [530/1000], loss:0.0029\n",
            "epoch [531/1000], loss:0.0032\n",
            "epoch [532/1000], loss:0.0031\n",
            "epoch [533/1000], loss:0.0031\n",
            "epoch [534/1000], loss:0.0030\n",
            "epoch [535/1000], loss:0.0028\n",
            "epoch [536/1000], loss:0.0029\n",
            "epoch [537/1000], loss:0.0036\n",
            "epoch [538/1000], loss:0.0029\n",
            "epoch [539/1000], loss:0.0029\n",
            "epoch [540/1000], loss:0.0047\n",
            "epoch [541/1000], loss:0.0029\n",
            "epoch [542/1000], loss:0.0026\n",
            "epoch [543/1000], loss:0.0029\n",
            "epoch [544/1000], loss:0.0032\n",
            "epoch [545/1000], loss:0.0032\n",
            "epoch [546/1000], loss:0.0029\n",
            "epoch [547/1000], loss:0.0033\n",
            "epoch [548/1000], loss:0.0027\n",
            "epoch [549/1000], loss:0.0030\n",
            "epoch [550/1000], loss:0.0026\n",
            "epoch [551/1000], loss:0.0027\n",
            "epoch [552/1000], loss:0.0026\n",
            "epoch [553/1000], loss:0.0026\n",
            "epoch [554/1000], loss:0.0027\n",
            "epoch [555/1000], loss:0.0028\n",
            "epoch [556/1000], loss:0.0030\n",
            "epoch [557/1000], loss:0.0028\n",
            "epoch [558/1000], loss:0.0027\n",
            "epoch [559/1000], loss:0.0030\n",
            "epoch [560/1000], loss:0.0031\n",
            "epoch [561/1000], loss:0.0030\n",
            "epoch [562/1000], loss:0.0036\n",
            "epoch [563/1000], loss:0.0034\n",
            "epoch [564/1000], loss:0.0030\n",
            "epoch [565/1000], loss:0.0028\n",
            "epoch [566/1000], loss:0.0027\n",
            "epoch [567/1000], loss:0.0029\n",
            "epoch [568/1000], loss:0.0025\n",
            "epoch [569/1000], loss:0.0026\n",
            "epoch [570/1000], loss:0.0028\n",
            "epoch [571/1000], loss:0.0028\n",
            "epoch [572/1000], loss:0.0024\n",
            "epoch [573/1000], loss:0.0028\n",
            "epoch [574/1000], loss:0.0028\n",
            "epoch [575/1000], loss:0.0039\n",
            "epoch [576/1000], loss:0.0025\n",
            "epoch [577/1000], loss:0.0027\n",
            "epoch [578/1000], loss:0.0031\n",
            "epoch [579/1000], loss:0.0029\n",
            "epoch [580/1000], loss:0.0030\n",
            "epoch [581/1000], loss:0.0034\n",
            "epoch [582/1000], loss:0.0026\n",
            "epoch [583/1000], loss:0.0033\n",
            "epoch [584/1000], loss:0.0027\n",
            "epoch [585/1000], loss:0.0032\n",
            "epoch [586/1000], loss:0.0028\n",
            "epoch [587/1000], loss:0.0028\n",
            "epoch [588/1000], loss:0.0027\n",
            "epoch [589/1000], loss:0.0030\n",
            "epoch [590/1000], loss:0.0024\n",
            "epoch [591/1000], loss:0.0029\n",
            "epoch [592/1000], loss:0.0031\n",
            "epoch [593/1000], loss:0.0034\n",
            "epoch [594/1000], loss:0.0031\n",
            "epoch [595/1000], loss:0.0030\n",
            "epoch [596/1000], loss:0.0032\n",
            "epoch [597/1000], loss:0.0027\n",
            "epoch [598/1000], loss:0.0028\n",
            "epoch [599/1000], loss:0.0030\n",
            "epoch [600/1000], loss:0.0034\n",
            "epoch [601/1000], loss:0.0039\n",
            "epoch [602/1000], loss:0.0029\n",
            "epoch [603/1000], loss:0.0026\n",
            "epoch [604/1000], loss:0.0034\n",
            "epoch [605/1000], loss:0.0028\n",
            "epoch [606/1000], loss:0.0031\n",
            "epoch [607/1000], loss:0.0029\n",
            "epoch [608/1000], loss:0.0032\n",
            "epoch [609/1000], loss:0.0028\n",
            "epoch [610/1000], loss:0.0030\n",
            "epoch [611/1000], loss:0.0029\n",
            "epoch [612/1000], loss:0.0028\n",
            "epoch [613/1000], loss:0.0028\n",
            "epoch [614/1000], loss:0.0028\n",
            "epoch [615/1000], loss:0.0029\n",
            "epoch [616/1000], loss:0.0029\n",
            "epoch [617/1000], loss:0.0025\n",
            "epoch [618/1000], loss:0.0030\n",
            "epoch [619/1000], loss:0.0033\n",
            "epoch [620/1000], loss:0.0029\n",
            "epoch [621/1000], loss:0.0028\n",
            "epoch [622/1000], loss:0.0028\n",
            "epoch [623/1000], loss:0.0029\n",
            "epoch [624/1000], loss:0.0030\n",
            "epoch [625/1000], loss:0.0032\n",
            "epoch [626/1000], loss:0.0026\n",
            "epoch [627/1000], loss:0.0034\n",
            "epoch [628/1000], loss:0.0024\n",
            "epoch [629/1000], loss:0.0027\n",
            "epoch [630/1000], loss:0.0028\n",
            "epoch [631/1000], loss:0.0028\n",
            "epoch [632/1000], loss:0.0027\n",
            "epoch [633/1000], loss:0.0028\n",
            "epoch [634/1000], loss:0.0029\n",
            "epoch [635/1000], loss:0.0025\n",
            "epoch [636/1000], loss:0.0026\n",
            "epoch [637/1000], loss:0.0031\n",
            "epoch [638/1000], loss:0.0029\n",
            "epoch [639/1000], loss:0.0028\n",
            "epoch [640/1000], loss:0.0031\n",
            "epoch [641/1000], loss:0.0029\n",
            "epoch [642/1000], loss:0.0029\n",
            "epoch [643/1000], loss:0.0028\n",
            "epoch [644/1000], loss:0.0025\n",
            "epoch [645/1000], loss:0.0028\n",
            "epoch [646/1000], loss:0.0024\n",
            "epoch [647/1000], loss:0.0035\n",
            "epoch [648/1000], loss:0.0030\n",
            "epoch [649/1000], loss:0.0026\n",
            "epoch [650/1000], loss:0.0026\n",
            "epoch [651/1000], loss:0.0030\n",
            "epoch [652/1000], loss:0.0029\n",
            "epoch [653/1000], loss:0.0030\n",
            "epoch [654/1000], loss:0.0028\n",
            "epoch [655/1000], loss:0.0031\n",
            "epoch [656/1000], loss:0.0035\n",
            "epoch [657/1000], loss:0.0032\n",
            "epoch [658/1000], loss:0.0028\n",
            "epoch [659/1000], loss:0.0032\n",
            "epoch [660/1000], loss:0.0027\n",
            "epoch [661/1000], loss:0.0029\n",
            "epoch [662/1000], loss:0.0026\n",
            "epoch [663/1000], loss:0.0028\n",
            "epoch [664/1000], loss:0.0032\n",
            "epoch [665/1000], loss:0.0029\n",
            "epoch [666/1000], loss:0.0026\n",
            "epoch [667/1000], loss:0.0027\n",
            "epoch [668/1000], loss:0.0028\n",
            "epoch [669/1000], loss:0.0031\n",
            "epoch [670/1000], loss:0.0030\n",
            "epoch [671/1000], loss:0.0029\n",
            "epoch [672/1000], loss:0.0028\n",
            "epoch [673/1000], loss:0.0030\n",
            "epoch [674/1000], loss:0.0030\n",
            "epoch [675/1000], loss:0.0027\n",
            "epoch [676/1000], loss:0.0026\n",
            "epoch [677/1000], loss:0.0033\n",
            "epoch [678/1000], loss:0.0025\n",
            "epoch [679/1000], loss:0.0026\n",
            "epoch [680/1000], loss:0.0032\n",
            "epoch [681/1000], loss:0.0032\n",
            "epoch [682/1000], loss:0.0028\n",
            "epoch [683/1000], loss:0.0030\n",
            "epoch [684/1000], loss:0.0029\n",
            "epoch [685/1000], loss:0.0024\n",
            "epoch [686/1000], loss:0.0030\n",
            "epoch [687/1000], loss:0.0029\n",
            "epoch [688/1000], loss:0.0030\n",
            "epoch [689/1000], loss:0.0031\n",
            "epoch [690/1000], loss:0.0031\n",
            "epoch [691/1000], loss:0.0028\n",
            "epoch [692/1000], loss:0.0031\n",
            "epoch [693/1000], loss:0.0030\n",
            "epoch [694/1000], loss:0.0029\n",
            "epoch [695/1000], loss:0.0034\n",
            "epoch [696/1000], loss:0.0027\n",
            "epoch [697/1000], loss:0.0027\n",
            "epoch [698/1000], loss:0.0029\n",
            "epoch [699/1000], loss:0.0030\n",
            "epoch [700/1000], loss:0.0025\n",
            "epoch [701/1000], loss:0.0029\n",
            "epoch [702/1000], loss:0.0027\n",
            "epoch [703/1000], loss:0.0037\n",
            "epoch [704/1000], loss:0.0027\n",
            "epoch [705/1000], loss:0.0034\n",
            "epoch [706/1000], loss:0.0031\n",
            "epoch [707/1000], loss:0.0024\n",
            "epoch [708/1000], loss:0.0029\n",
            "epoch [709/1000], loss:0.0026\n",
            "epoch [710/1000], loss:0.0031\n",
            "epoch [711/1000], loss:0.0032\n",
            "epoch [712/1000], loss:0.0026\n",
            "epoch [713/1000], loss:0.0036\n",
            "epoch [714/1000], loss:0.0025\n",
            "epoch [715/1000], loss:0.0029\n",
            "epoch [716/1000], loss:0.0030\n",
            "epoch [717/1000], loss:0.0028\n",
            "epoch [718/1000], loss:0.0035\n",
            "epoch [719/1000], loss:0.0029\n",
            "epoch [720/1000], loss:0.0028\n",
            "epoch [721/1000], loss:0.0029\n",
            "epoch [722/1000], loss:0.0029\n",
            "epoch [723/1000], loss:0.0029\n",
            "epoch [724/1000], loss:0.0035\n",
            "epoch [725/1000], loss:0.0030\n",
            "epoch [726/1000], loss:0.0031\n",
            "epoch [727/1000], loss:0.0026\n",
            "epoch [728/1000], loss:0.0028\n",
            "epoch [729/1000], loss:0.0029\n",
            "epoch [730/1000], loss:0.0035\n",
            "epoch [731/1000], loss:0.0028\n",
            "epoch [732/1000], loss:0.0027\n",
            "epoch [733/1000], loss:0.0030\n",
            "epoch [734/1000], loss:0.0026\n",
            "epoch [735/1000], loss:0.0031\n",
            "epoch [736/1000], loss:0.0028\n",
            "epoch [737/1000], loss:0.0025\n",
            "epoch [738/1000], loss:0.0028\n",
            "epoch [739/1000], loss:0.0027\n",
            "epoch [740/1000], loss:0.0029\n",
            "epoch [741/1000], loss:0.0030\n",
            "epoch [742/1000], loss:0.0027\n",
            "epoch [743/1000], loss:0.0033\n",
            "epoch [744/1000], loss:0.0030\n",
            "epoch [745/1000], loss:0.0029\n",
            "epoch [746/1000], loss:0.0031\n",
            "epoch [747/1000], loss:0.0033\n",
            "epoch [748/1000], loss:0.0032\n",
            "epoch [749/1000], loss:0.0032\n",
            "epoch [750/1000], loss:0.0028\n",
            "epoch [751/1000], loss:0.0030\n",
            "epoch [752/1000], loss:0.0033\n",
            "epoch [753/1000], loss:0.0029\n",
            "epoch [754/1000], loss:0.0026\n",
            "epoch [755/1000], loss:0.0033\n",
            "epoch [756/1000], loss:0.0029\n",
            "epoch [757/1000], loss:0.0027\n",
            "epoch [758/1000], loss:0.0027\n",
            "epoch [759/1000], loss:0.0030\n",
            "epoch [760/1000], loss:0.0027\n",
            "epoch [761/1000], loss:0.0033\n",
            "epoch [762/1000], loss:0.0032\n",
            "epoch [763/1000], loss:0.0029\n",
            "epoch [764/1000], loss:0.0027\n",
            "epoch [765/1000], loss:0.0031\n",
            "epoch [766/1000], loss:0.0028\n",
            "epoch [767/1000], loss:0.0033\n",
            "epoch [768/1000], loss:0.0033\n",
            "epoch [769/1000], loss:0.0032\n",
            "epoch [770/1000], loss:0.0033\n",
            "epoch [771/1000], loss:0.0029\n",
            "epoch [772/1000], loss:0.0033\n",
            "epoch [773/1000], loss:0.0031\n",
            "epoch [774/1000], loss:0.0026\n",
            "epoch [775/1000], loss:0.0028\n",
            "epoch [776/1000], loss:0.0029\n",
            "epoch [777/1000], loss:0.0030\n",
            "epoch [778/1000], loss:0.0026\n",
            "epoch [779/1000], loss:0.0029\n",
            "epoch [780/1000], loss:0.0032\n",
            "epoch [781/1000], loss:0.0028\n",
            "epoch [782/1000], loss:0.0031\n",
            "epoch [783/1000], loss:0.0028\n",
            "epoch [784/1000], loss:0.0028\n",
            "epoch [785/1000], loss:0.0026\n",
            "epoch [786/1000], loss:0.0030\n",
            "epoch [787/1000], loss:0.0030\n",
            "epoch [788/1000], loss:0.0031\n",
            "epoch [789/1000], loss:0.0028\n",
            "epoch [790/1000], loss:0.0027\n",
            "epoch [791/1000], loss:0.0029\n",
            "epoch [792/1000], loss:0.0029\n",
            "epoch [793/1000], loss:0.0025\n",
            "epoch [794/1000], loss:0.0026\n",
            "epoch [795/1000], loss:0.0034\n",
            "epoch [796/1000], loss:0.0030\n",
            "epoch [797/1000], loss:0.0030\n",
            "epoch [798/1000], loss:0.0028\n",
            "epoch [799/1000], loss:0.0028\n",
            "epoch [800/1000], loss:0.0032\n",
            "epoch [801/1000], loss:0.0028\n",
            "epoch [802/1000], loss:0.0034\n",
            "epoch [803/1000], loss:0.0031\n",
            "epoch [804/1000], loss:0.0029\n",
            "epoch [805/1000], loss:0.0030\n",
            "epoch [806/1000], loss:0.0026\n",
            "epoch [807/1000], loss:0.0034\n",
            "epoch [808/1000], loss:0.0031\n",
            "epoch [809/1000], loss:0.0025\n",
            "epoch [810/1000], loss:0.0035\n",
            "epoch [811/1000], loss:0.0034\n",
            "epoch [812/1000], loss:0.0026\n",
            "epoch [813/1000], loss:0.0028\n",
            "epoch [814/1000], loss:0.0028\n",
            "epoch [815/1000], loss:0.0027\n",
            "epoch [816/1000], loss:0.0028\n",
            "epoch [817/1000], loss:0.0029\n",
            "epoch [818/1000], loss:0.0027\n",
            "epoch [819/1000], loss:0.0059\n",
            "epoch [820/1000], loss:0.0027\n",
            "epoch [821/1000], loss:0.0029\n",
            "epoch [822/1000], loss:0.0025\n",
            "epoch [823/1000], loss:0.0025\n",
            "epoch [824/1000], loss:0.0032\n",
            "epoch [825/1000], loss:0.0036\n",
            "epoch [826/1000], loss:0.0031\n",
            "epoch [827/1000], loss:0.0032\n",
            "epoch [828/1000], loss:0.0025\n",
            "epoch [829/1000], loss:0.0026\n",
            "epoch [830/1000], loss:0.0032\n",
            "epoch [831/1000], loss:0.0032\n",
            "epoch [832/1000], loss:0.0028\n",
            "epoch [833/1000], loss:0.0032\n",
            "epoch [834/1000], loss:0.0031\n",
            "epoch [835/1000], loss:0.0035\n",
            "epoch [836/1000], loss:0.0030\n",
            "epoch [837/1000], loss:0.0026\n",
            "epoch [838/1000], loss:0.0025\n",
            "epoch [839/1000], loss:0.0029\n",
            "epoch [840/1000], loss:0.0040\n",
            "epoch [841/1000], loss:0.0026\n",
            "epoch [842/1000], loss:0.0029\n",
            "epoch [843/1000], loss:0.0029\n",
            "epoch [844/1000], loss:0.0027\n",
            "epoch [845/1000], loss:0.0027\n",
            "epoch [846/1000], loss:0.0032\n",
            "epoch [847/1000], loss:0.0025\n",
            "epoch [848/1000], loss:0.0026\n",
            "epoch [849/1000], loss:0.0029\n",
            "epoch [850/1000], loss:0.0025\n",
            "epoch [851/1000], loss:0.0031\n",
            "epoch [852/1000], loss:0.0032\n",
            "epoch [853/1000], loss:0.0033\n",
            "epoch [854/1000], loss:0.0029\n",
            "epoch [855/1000], loss:0.0030\n",
            "epoch [856/1000], loss:0.0027\n",
            "epoch [857/1000], loss:0.0025\n",
            "epoch [858/1000], loss:0.0031\n",
            "epoch [859/1000], loss:0.0025\n",
            "epoch [860/1000], loss:0.0029\n",
            "epoch [861/1000], loss:0.0032\n",
            "epoch [862/1000], loss:0.0030\n",
            "epoch [863/1000], loss:0.0030\n",
            "epoch [864/1000], loss:0.0026\n",
            "epoch [865/1000], loss:0.0025\n",
            "epoch [866/1000], loss:0.0025\n",
            "epoch [867/1000], loss:0.0030\n",
            "epoch [868/1000], loss:0.0029\n",
            "epoch [869/1000], loss:0.0032\n",
            "epoch [870/1000], loss:0.0027\n",
            "epoch [871/1000], loss:0.0026\n",
            "epoch [872/1000], loss:0.0031\n",
            "epoch [873/1000], loss:0.0026\n",
            "epoch [874/1000], loss:0.0029\n",
            "epoch [875/1000], loss:0.0029\n",
            "epoch [876/1000], loss:0.0031\n",
            "epoch [877/1000], loss:0.0037\n",
            "epoch [878/1000], loss:0.0027\n",
            "epoch [879/1000], loss:0.0030\n",
            "epoch [880/1000], loss:0.0027\n",
            "epoch [881/1000], loss:0.0031\n",
            "epoch [882/1000], loss:0.0027\n",
            "epoch [883/1000], loss:0.0027\n",
            "epoch [884/1000], loss:0.0027\n",
            "epoch [885/1000], loss:0.0032\n",
            "epoch [886/1000], loss:0.0033\n",
            "epoch [887/1000], loss:0.0025\n",
            "epoch [888/1000], loss:0.0027\n",
            "epoch [889/1000], loss:0.0029\n",
            "epoch [890/1000], loss:0.0028\n",
            "epoch [891/1000], loss:0.0027\n",
            "epoch [892/1000], loss:0.0029\n",
            "epoch [893/1000], loss:0.0024\n",
            "epoch [894/1000], loss:0.0028\n",
            "epoch [895/1000], loss:0.0029\n",
            "epoch [896/1000], loss:0.0024\n",
            "epoch [897/1000], loss:0.0028\n",
            "epoch [898/1000], loss:0.0028\n",
            "epoch [899/1000], loss:0.0030\n",
            "epoch [900/1000], loss:0.0027\n",
            "epoch [901/1000], loss:0.0028\n",
            "epoch [902/1000], loss:0.0033\n",
            "epoch [903/1000], loss:0.0027\n",
            "epoch [904/1000], loss:0.0027\n",
            "epoch [905/1000], loss:0.0028\n",
            "epoch [906/1000], loss:0.0032\n",
            "epoch [907/1000], loss:0.0030\n",
            "epoch [908/1000], loss:0.0028\n",
            "epoch [909/1000], loss:0.0030\n",
            "epoch [910/1000], loss:0.0028\n",
            "epoch [911/1000], loss:0.0030\n",
            "epoch [912/1000], loss:0.0028\n",
            "epoch [913/1000], loss:0.0032\n",
            "epoch [914/1000], loss:0.0031\n",
            "epoch [915/1000], loss:0.0030\n",
            "epoch [916/1000], loss:0.0029\n",
            "epoch [917/1000], loss:0.0032\n",
            "epoch [918/1000], loss:0.0031\n",
            "epoch [919/1000], loss:0.0029\n",
            "epoch [920/1000], loss:0.0030\n",
            "epoch [921/1000], loss:0.0029\n",
            "epoch [922/1000], loss:0.0027\n",
            "epoch [923/1000], loss:0.0028\n",
            "epoch [924/1000], loss:0.0032\n",
            "epoch [925/1000], loss:0.0028\n",
            "epoch [926/1000], loss:0.0026\n",
            "epoch [927/1000], loss:0.0030\n",
            "epoch [928/1000], loss:0.0031\n",
            "epoch [929/1000], loss:0.0027\n",
            "epoch [930/1000], loss:0.0030\n",
            "epoch [931/1000], loss:0.0028\n",
            "epoch [932/1000], loss:0.0026\n",
            "epoch [933/1000], loss:0.0029\n",
            "epoch [934/1000], loss:0.0028\n",
            "epoch [935/1000], loss:0.0028\n",
            "epoch [936/1000], loss:0.0026\n",
            "epoch [937/1000], loss:0.0033\n",
            "epoch [938/1000], loss:0.0025\n",
            "epoch [939/1000], loss:0.0027\n",
            "epoch [940/1000], loss:0.0025\n",
            "epoch [941/1000], loss:0.0027\n",
            "epoch [942/1000], loss:0.0037\n",
            "epoch [943/1000], loss:0.0026\n",
            "epoch [944/1000], loss:0.0028\n",
            "epoch [945/1000], loss:0.0028\n",
            "epoch [946/1000], loss:0.0033\n",
            "epoch [947/1000], loss:0.0027\n",
            "epoch [948/1000], loss:0.0026\n",
            "epoch [949/1000], loss:0.0028\n",
            "epoch [950/1000], loss:0.0032\n",
            "epoch [951/1000], loss:0.0029\n",
            "epoch [952/1000], loss:0.0027\n",
            "epoch [953/1000], loss:0.0033\n",
            "epoch [954/1000], loss:0.0025\n",
            "epoch [955/1000], loss:0.0031\n",
            "epoch [956/1000], loss:0.0025\n",
            "epoch [957/1000], loss:0.0026\n",
            "epoch [958/1000], loss:0.0029\n",
            "epoch [959/1000], loss:0.0028\n",
            "epoch [960/1000], loss:0.0029\n",
            "epoch [961/1000], loss:0.0027\n",
            "epoch [962/1000], loss:0.0027\n",
            "epoch [963/1000], loss:0.0028\n",
            "epoch [964/1000], loss:0.0027\n",
            "epoch [965/1000], loss:0.0031\n",
            "epoch [966/1000], loss:0.0036\n",
            "epoch [967/1000], loss:0.0029\n",
            "epoch [968/1000], loss:0.0030\n",
            "epoch [969/1000], loss:0.0033\n",
            "epoch [970/1000], loss:0.0027\n",
            "epoch [971/1000], loss:0.0030\n",
            "epoch [972/1000], loss:0.0028\n",
            "epoch [973/1000], loss:0.0032\n",
            "epoch [974/1000], loss:0.0031\n",
            "epoch [975/1000], loss:0.0030\n",
            "epoch [976/1000], loss:0.0026\n",
            "epoch [977/1000], loss:0.0028\n",
            "epoch [978/1000], loss:0.0028\n",
            "epoch [979/1000], loss:0.0028\n",
            "epoch [980/1000], loss:0.0031\n",
            "epoch [981/1000], loss:0.0027\n",
            "epoch [982/1000], loss:0.0027\n",
            "epoch [983/1000], loss:0.0036\n",
            "epoch [984/1000], loss:0.0029\n",
            "epoch [985/1000], loss:0.0032\n",
            "epoch [986/1000], loss:0.0029\n",
            "epoch [987/1000], loss:0.0032\n",
            "epoch [988/1000], loss:0.0029\n",
            "epoch [989/1000], loss:0.0024\n",
            "epoch [990/1000], loss:0.0029\n",
            "epoch [991/1000], loss:0.0029\n",
            "epoch [992/1000], loss:0.0025\n",
            "epoch [993/1000], loss:0.0033\n",
            "epoch [994/1000], loss:0.0031\n",
            "epoch [995/1000], loss:0.0031\n",
            "epoch [996/1000], loss:0.0030\n",
            "epoch [997/1000], loss:0.0028\n",
            "epoch [998/1000], loss:0.0025\n",
            "epoch [999/1000], loss:0.0025\n",
            "epoch [1000/1000], loss:0.0026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk0UxFuchLzR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku_wpuheWWNz",
        "colab_type": "text"
      },
      "source": [
        "將 testing 的圖片輸入 model 後，可以得到其重建的圖片，並對兩者取平方差。可以發現 inlier 的平方差應該與 outlier 的平方差形成差距明顯的兩群數據。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1PS_ApzhfOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if task == 'ae':\n",
        "    if model_type == 'fcn' or model_type == 'vae':\n",
        "        y = test.reshape(len(test), -1)\n",
        "    else:\n",
        "        y = test\n",
        "        \n",
        "    data = torch.tensor(y, dtype=torch.float)\n",
        "    test_dataset = TensorDataset(data)\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "    model = torch.load('best_model_{}.pt'.format(model_type), map_location='cuda')\n",
        "\n",
        "    model.eval()\n",
        "    reconstructed = list()\n",
        "    for i, data in enumerate(test_dataloader): \n",
        "        if model_type == 'cnn':\n",
        "            img = data[0].transpose(3, 1).cuda()\n",
        "        else:\n",
        "            img = data[0].cuda()\n",
        "        output = model(img)\n",
        "        if model_type == 'cnn':\n",
        "            output = output.transpose(3, 1)\n",
        "        elif model_type == 'vae':\n",
        "            output = output[0]\n",
        "        reconstructed.append(output.cpu().detach().numpy())\n",
        "\n",
        "    reconstructed = np.concatenate(reconstructed, axis=0)\n",
        "    anomality = np.sqrt(np.sum(np.square(reconstructed - y).reshape(len(y), -1), axis=1))\n",
        "    y_pred = anomality\n",
        "    with open('prediction.csv', 'w') as f:\n",
        "        f.write('id,anomaly\\n')\n",
        "        for i in range(len(y_pred)):\n",
        "            f.write('{},{}\\n'.format(i+1, y_pred[i]))\n",
        "    # score = roc_auc_score(y_label, y_pred, average='micro')\n",
        "    # score = f1_score(y_label, y_pred, average='micro')\n",
        "    # print('auc score: {}'.format(score))\n"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}